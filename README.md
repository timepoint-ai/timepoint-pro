# Timepoint-Daedalus

**Temporal Knowledge Graph System with LLM-Driven Entity Simulation**

A sophisticated framework for creating queryable temporal simulations where entities evolve through causally-linked timepoints with adaptive fidelity and modal causality support.

---

## Overview

Timepoint-Daedalus enables you to:
- **Generate simulations from natural language** - "Simulate the Constitutional Convention of 1787"
- **Query across time and entities** - Track knowledge flow, relationships, and decisions
- **Export results** - Reports in Markdown, JSON, CSV with compression
- **Optimize costs** - 95% reduction via adaptive fidelity (tensor compression)

**Status**: Validated and Production Ready ✅
- All 17 core mechanisms implemented (M1-M17)
- 462 tests collected (17 E2E, 445 integration/unit)
- Real LLM integration validated (requires `OPENROUTER_API_KEY`)
- Complete pipeline: Natural Language → Simulation → Query → Report → Export
- **Note**: Mock mode disabled - system requires real LLM for production workflows
- **Validation**: See [VALIDATION_REPORT.md](VALIDATION_REPORT.md) for comprehensive validation evidence

---

## Quick Start

### Installation

```bash
git clone https://github.com/yourusername/timepoint-daedalus.git
cd timepoint-daedalus
pip install -r requirements.txt
```

### Configuration

```bash
# Set up your API key
export OPENROUTER_API_KEY=your_key_here
export LLM_SERVICE_ENABLED=true
```

### Basic Usage

```python
from nl_interface import NLConfigGenerator
from orchestrator import simulate_event
from llm_v2 import LLMClient
from storage import GraphStore

# 1. Generate config from natural language
generator = NLConfigGenerator()
config, confidence = generator.generate_config(
    "Simulate a board meeting with 5 executives discussing an acquisition. "
    "Focus on dialog and decision making."
)

# 2. Execute simulation
llm = LLMClient()
store = GraphStore("sqlite:///simulations.db")

result = simulate_event(
    config['scenario'],
    llm,
    store,
    context={
        "max_entities": len(config['entities']),
        "max_timepoints": 5,
        "temporal_mode": "pearl"
    },
    save_to_db=True
)

# 3. Query results
from reporting.query_engine import EnhancedQueryEngine

query_engine = EnhancedQueryEngine()
world_id = f"simulation_{result['timepoints'][0].timepoint_id}"

relationships = query_engine.summarize_relationships(world_id)
timeline = query_engine.timeline_summary(world_id)
knowledge_flow = query_engine.knowledge_flow_graph(world_id)

# 4. Generate reports
from reporting.report_generator import ReportGenerator

report_gen = ReportGenerator(query_engine)
markdown_report = report_gen.generate_summary_report(
    world_id=world_id,
    format="markdown"
)

# 5. Export data
from reporting.export_pipeline import ExportPipeline

exporter = ExportPipeline(query_engine)
exporter.export_report(
    world_id=world_id,
    report_type="summary",
    export_format="json",
    output_path="./output/summary.json"
)
```

---

## Complete Pipeline

The system provides an end-to-end workflow:

```
Natural Language Description
         ↓
    Config Generation (Sprint 3)
         ↓
    Simulation Execution (Orchestrator)
         ↓
    Query Interface (Sprint 1)
         ↓
    Report Generation (Sprint 2)
         ↓
    Data Export (Sprint 2)
         ↓
    Exported Files
```

See `test_e2e_complete_pipeline.py` for a working example.

---

## Key Features

### 1. Natural Language Interface

Convert plain English into executable simulation configs:

```python
from nl_interface import NLConfigGenerator

generator = NLConfigGenerator()
config, confidence = generator.generate_config(
    "Simulate Paul Revere's midnight ride. 8 timepoints. "
    "Focus on knowledge propagation."
)
# Returns validated config with 80-100% confidence
```

### 2. Modal Temporal Causality

Choose from five temporal modes:

- **Pearl** - Standard DAG causality (historical realism)
- **Directorial** - Narrative-driven (dramatic coherence)
- **Nonlinear** - Flashbacks and non-linear presentation
- **Branching** - Many-worlds counterfactuals
- **Cyclical** - Time loops and prophecy

### 3. Adaptive Fidelity

Five resolution levels optimize cost vs. detail:

1. **TENSOR** - Compressed (200 tokens, ~$0.01)
2. **SCENE** - Context (~1-2k tokens)
3. **GRAPH** - Relationships (~5k tokens)
4. **DIALOG** - Conversations (~10k tokens)
5. **TRAINED** - Full state (~50k tokens)

**Result**: 95% cost reduction vs. uniform high-fidelity.

### 4. Animistic Entities

Support for non-human entities:

- Animals (biological constraints)
- Buildings (structural integrity)
- Objects (state tracking)
- Abstract concepts (idea propagation)
- AI entities (external agent integration)

### 5. Query & Export

Comprehensive querying with multiple export formats:

```python
from reporting.query_engine import EnhancedQueryEngine
from reporting.export_pipeline import ExportPipeline

# Query
engine = EnhancedQueryEngine()
relationships = engine.summarize_relationships(world_id)
knowledge_flow = engine.knowledge_flow_graph(world_id)

# Export
exporter = ExportPipeline(engine)
exporter.export_batch(
    world_id=world_id,
    report_types=["summary", "relationships", "knowledge"],
    export_formats=["json", "markdown"],
    output_dir="./exports/"
)
```

**Supported Formats**: JSON, JSONL, Markdown, CSV, SQLite
**Compression**: gzip, bz2 (50-70% size reduction)

---

## Testing

### Run Tests

```bash
# All tests (462 tests collected)
pytest -v

# E2E tests (17 tests - require real LLM)
pytest test_e2e_autopilot.py -v --real-llm

# Validation workflow tests (4 critical workflows)
pytest test_e2e_autopilot.py::TestE2EValidationWorkflows -v --real-llm

# Unit/integration tests (run without API key)
pytest -m "not llm" -v

# Run with real LLM (requires OPENROUTER_API_KEY)
export OPENROUTER_API_KEY=your_key
pytest -m llm -v

# Complete pipeline integration
pytest test_e2e_complete_pipeline.py -v -s
```

### Test Coverage

**Total Tests**: 462 collected

| Test Type | Count | Notes |
|-----------|-------|-------|
| E2E (full stack) | 17 | Require real LLM (13 original + 4 validation workflows) |
| Integration | ~200 | Multi-component tests |
| Unit | ~245 | Isolated components |
| **TOTAL** | **462** | See [TEST_CHECKLIST.md](TEST_CHECKLIST.md) for details |

**Test Markers**:
- `@pytest.mark.e2e` - End-to-end workflows
- `@pytest.mark.llm` - Require real LLM API
- `@pytest.mark.validation` - Critical validation workflow tests
- `@pytest.mark.slow` - Long-running tests
- `@pytest.mark.unit` - Fast isolated tests

**Validation Workflows** (see [VALIDATION_REPORT.md](VALIDATION_REPORT.md)):
1. Timepoint AI modeling (orchestrator → LLM → scene spec)
2. E2E test rig with real LLM
3. Data generation + Oxen storage
4. Fine-tuning data quality validation

---

## Fine-Tuning

### Horizontal Fine-Tuning (Breadth)
Generate many scenario variations for training diversity:

```bash
# Generate 50 variations with different personalities/outcomes
python run_real_finetune.py
```

### Vertical Fine-Tuning (Depth)
Generate deep temporal simulations with 12 timepoints:

```bash
# Generate deep temporal simulation with progressive training
python run_vertical_finetune.py
```

**Output**: Training data in JSONL format uploaded to Oxen.ai
**Repository**: `realityinspector/proof-the-e2e-works`

**Note**: Oxen.ai does not support programmatic fine-tune creation. After uploading:
1. Visit your Oxen repository
2. Navigate to the uploaded dataset file
3. Use the web UI to manually create a fine-tuning job

---

## Documentation

- **README.md** (this file) - Quick start and overview
- **MECHANICS.md** - Technical architecture and 17 core mechanisms
- **VALIDATION_REPORT.md** - Comprehensive validation evidence
- **TEST_CHECKLIST.md** - Test inventory and validation checklist

---

## Performance

### Benchmarks

- **Small simulation** (5 entities, 5 timepoints): $1-2, 30-60s
- **Medium simulation** (20 entities, 10 timepoints): $5-8, 60-120s
- **Large simulation** (100 entities, 20 timepoints): $20-30, 120-300s

### Efficiency

- **Cost reduction**: 95% (tensor compression + resolution optimization)
- **Compression ratio**: 97% (50k tokens → 200 tokens via TTM)
- **Query caching**: LRU with TTL (600s default)

---

## Architecture

### Core Components

**Natural Language Interface** (`nl_interface/`):
- `nl_to_config.py` - NL → Config translation
- `interactive_refiner.py` - Interactive refinement
- `clarification_engine.py` - Ambiguity detection
- `config_validator.py` - Validation pipeline

**Query & Reporting** (`reporting/`):
- `query_engine.py` - Enhanced query interface
- `report_generator.py` - Multi-format reports
- `export_pipeline.py` - Batch export orchestration
- `formatters.py` - Format conversion
- `export_formats.py` - Export handlers

**Timepoint Stack**:
- `orchestrator.py` - Scene orchestration (742 lines)
- `llm_v2.py` - LLM client with retry logic (1,000 lines)
- `storage.py` - SQLite persistence
- `schemas.py` - Pydantic V2 models (529 lines)
- `temporal_causality.py` - Pearl-mode causality
- `validation.py` - Safety & validation (1,340 lines)

**Generation** (`generation/`):
- `world_manager.py` - Simulation world management
- `horizontal_generator.py` - Variation generation
- `vertical_generator.py` - Temporal depth expansion
- `progress_tracker.py` - Real-time metrics
- `fault_handler.py` - Error recovery
- `checkpoint_manager.py` - State persistence

---

## Dependencies

**Core**:
- `langgraph>=0.2.62` - Workflow orchestration
- `networkx>=3.4.2` - Graph operations
- `instructor>=1.7.0` - LLM structured outputs
- `httpx>=0.27.0` - HTTP client
- `sqlmodel>=0.0.22` - ORM
- `pydantic>=2.10.0` - Validation
- `numpy>=2.2.1`, `scipy>=1.15.0` - Tensor operations

**Testing**:
- `pytest>=8.3.4` - Test framework
- `pytest-asyncio>=0.25.2` - Async support
- `pytest-cov>=6.0.0` - Coverage

See `requirements.txt` for complete list.

---

## License

MIT

---

## Contact

For questions or issues, please open a GitHub issue.

---

**Validated and Production Ready** ✅ | **462 Tests (17 E2E)** ✅ | **Real LLM Integration** ✅ | See **VALIDATION_REPORT.md** for evidence
