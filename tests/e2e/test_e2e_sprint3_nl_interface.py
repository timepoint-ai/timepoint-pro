"""
End-to-End Tests for Sprint 3: Natural Language Interface

Tests the complete workflow from natural language description to
validated simulation configuration, including interactive refinement.
"""

import pytest
from nl_interface import (
    NLConfigGenerator,
    InteractiveRefiner,
    ClarificationEngine
)


class TestE2ENLToConfig:
    """E2E tests for NL → Config translation"""

    def test_simple_board_meeting_config_generation(self):
        """Test generating config from simple board meeting description"""
        generator = NLConfigGenerator()  # Mock mode

        description = "Simulate a board meeting with 5 executives. 10 timepoints. Focus on dialog and decision making."

        config, confidence = generator.generate_config(description)

        # Verify config structure
        assert "scenario" in config
        assert "entities" in config
        assert "timepoint_count" in config
        assert "temporal_mode" in config
        assert "focus" in config
        assert "outputs" in config

        # Verify mock generated correct entity count
        assert len(config["entities"]) == 5

        # Verify timepoint count
        assert config["timepoint_count"] == 10

        # Verify focus areas
        assert "dialog" in config["focus"]
        assert "decision_making" in config["focus"]

        # Verify confidence
        assert confidence > 0.0

    def test_historical_scenario_config_generation(self):
        """Test generating config for historical scenario"""
        generator = NLConfigGenerator()

        description = (
            "Simulate the Constitutional Convention with 10 delegates. "
            "15 timepoints covering key debates. "
            "Focus on dialog, decision making, and knowledge propagation."
        )

        config, confidence = generator.generate_config(description)

        # Verify entity count
        assert len(config["entities"]) == 10

        # Verify timepoint count
        assert config["timepoint_count"] == 15

        # Verify focus areas
        assert "dialog" in config["focus"]
        assert "decision_making" in config["focus"]

    def test_config_validation_workflow(self):
        """Test complete validation workflow"""
        generator = NLConfigGenerator()

        # Generate config
        description = "Simulate a crisis meeting with 3 people. 5 timepoints. Focus on stress and decisions."
        config, confidence = generator.generate_config(description)

        # Validate config
        validation = generator.validate_config(config)

        # Should be valid
        assert validation.is_valid

        # Should have high confidence
        assert validation.confidence_score > 0.5

        # Check for warnings (optional)
        # Warnings are fine, just check structure
        assert isinstance(validation.warnings, list)
        assert isinstance(validation.errors, list)

    def test_invalid_config_detection(self):
        """Test validation catches invalid configurations"""
        generator = NLConfigGenerator()

        # Create invalid config (too many timepoints)
        invalid_config = {
            "scenario": "Test",
            "entities": [{"name": "Alice", "role": "CEO"}],
            "timepoint_count": 101,  # Exceeds maximum
            "temporal_mode": "pearl",
            "focus": ["dialog"],
            "outputs": ["dialog"]
        }

        validation = generator.validate_config(invalid_config)

        # Should be invalid
        assert not validation.is_valid
        assert len(validation.errors) > 0

    def test_confidence_scoring(self):
        """Test confidence scoring for different quality configs"""
        generator = NLConfigGenerator()

        # High quality description
        high_quality = (
            "Simulate a board meeting with 5 executives. "
            "10 timepoints. Focus on dialog and decision making. "
            "Output: dialog and decisions."
        )

        _, high_confidence = generator.generate_config(high_quality)

        # Should have reasonable confidence
        assert high_confidence >= 0.5

        # Get confidence explanation
        explanation = generator.get_confidence_explanation(high_confidence)
        assert isinstance(explanation, str)
        assert len(explanation) > 0


class TestE2EInteractiveRefinement:
    """E2E tests for interactive refinement workflow"""

    def test_complete_refinement_workflow(self):
        """Test complete interactive refinement from start to approval"""
        refiner = InteractiveRefiner()

        # 1. Start refinement
        description = "Simulate a board meeting with 5 executives. 10 timepoints. Focus on dialog."
        result = refiner.start_refinement(description, skip_clarifications=True)

        # Should generate config
        assert result["config"] is not None
        assert result["validation"] is not None

        # 2. Preview config
        preview = refiner.preview_config(format="summary")
        assert "Scenario" in preview
        assert "Entities" in preview

        # 3. Approve config
        final_config = refiner.approve_config()

        # Should return valid config
        assert final_config is not None
        assert "scenario" in final_config
        assert "entities" in final_config

    def test_clarification_detection_and_resolution(self):
        """Test clarification detection and resolution workflow"""
        refiner = InteractiveRefiner()

        # Start with incomplete description
        incomplete_description = "Simulate a board meeting"

        result = refiner.start_refinement(incomplete_description)

        # Should detect missing information
        assert result["clarifications_needed"] is True
        assert len(result["clarifications"]) > 0

        # Should have no config yet
        assert result["config"] is None

        # Answer clarifications
        answers = {
            "entity_count": "5",
            "timepoint_count": "10",
            "focus": "dialog, decision_making"
        }

        result = refiner.answer_clarifications(answers)

        # Should now have config
        assert result["config"] is not None
        assert result["validation"] is not None

    def test_config_adjustment_workflow(self):
        """Test adjusting configuration parameters"""
        refiner = InteractiveRefiner()

        # Generate initial config
        refiner.start_refinement(
            "Simulate a meeting with 5 people. 10 timepoints. Focus on dialog.",
            skip_clarifications=True
        )

        # Get initial timepoint count
        initial_config = refiner.current_config
        initial_timepoints = initial_config["timepoint_count"]

        # Adjust timepoint count
        result = refiner.adjust_config(
            {"timepoint_count": 15},
            regenerate=False
        )

        # Should have updated config
        assert result["config"]["timepoint_count"] == 15
        assert result["config"]["timepoint_count"] != initial_timepoints

    def test_rejection_and_restart_workflow(self):
        """Test rejecting config and restarting workflow"""
        refiner = InteractiveRefiner()

        # Generate initial config
        refiner.start_refinement(
            "Simulate a meeting with 5 people. 10 timepoints.",
            skip_clarifications=True
        )

        first_config = refiner.current_config

        # Reject and restart
        result = refiner.reject_and_restart(reason="Want different scenario")

        # Config should be cleared
        assert refiner.current_config is None or refiner.current_config != first_config

        # Refinement history should record rejection
        history = refiner.get_refinement_history()
        rejection_steps = [s for s in history if s.step_type == "rejection"]
        assert len(rejection_steps) > 0

    def test_refinement_trace_export(self):
        """Test exporting complete refinement trace"""
        refiner = InteractiveRefiner()

        # Go through refinement workflow
        refiner.start_refinement(
            "Simulate a board meeting with 5 executives. 10 timepoints.",
            skip_clarifications=True
        )

        refiner.adjust_config({"timepoint_count": 15}, regenerate=False)

        final_config = refiner.approve_config()

        # Export trace
        trace = refiner.export_refinement_trace()

        # Should have complete trace
        assert "original_description" in trace
        assert "final_description" in trace
        assert "final_config" in trace
        assert "final_validation" in trace
        assert "steps" in trace

        # Should have multiple steps
        assert len(trace["steps"]) >= 3  # start + generation + approval

        # Final config should match
        assert trace["final_config"] == final_config


class TestE2EClarificationEngine:
    """E2E tests for clarification engine"""

    def test_ambiguity_detection_comprehensive(self):
        """Test comprehensive ambiguity detection"""
        engine = ClarificationEngine()

        # Very incomplete description
        vague_description = "Simulate something"

        clarifications = engine.detect_ambiguities(vague_description)

        # Should detect multiple missing pieces
        assert len(clarifications) >= 3

        # Should have critical clarifications
        critical = [c for c in clarifications if c.priority == 1]
        assert len(critical) >= 1

        # Check for entity and timepoint count
        fields = [c.field for c in clarifications]
        assert "entity_count" in fields
        assert "timepoint_count" in fields

    def test_historical_scenario_detection(self):
        """Test historical scenario detection triggers appropriate clarifications"""
        engine = ClarificationEngine()

        historical = "Simulate the Apollo 13 crisis"

        clarifications = engine.detect_ambiguities(historical)

        # Should detect it's historical
        # Check if start_time clarification is present
        fields = [c.field for c in clarifications]

        # Historical scenarios should ask for start_time
        # (though it might not if "Apollo 13" isn't recognized - check the keyword list)
        # For now, just verify clarifications are generated
        assert len(clarifications) > 0

    def test_animism_detection(self):
        """Test detection of non-human entities requiring animism"""
        engine = ClarificationEngine()

        with_horse = "Simulate Paul Revere's midnight ride with his horse"

        clarifications = engine.detect_ambiguities(with_horse)

        # Should detect need for animism
        fields = [c.field for c in clarifications]
        assert "animism_level" in fields

    def test_variation_generation_detection(self):
        """Test detection of horizontal/variation generation requests"""
        engine = ClarificationEngine()

        # Without count specified - should ask for clarification
        variations_no_count = "Generate variations of a job interview"

        clarifications = engine.detect_ambiguities(variations_no_count)

        # Should detect variation request and ask for count
        fields = [c.field for c in clarifications]
        assert "variation_count" in fields

    def test_clarification_summary_generation(self):
        """Test generating human-readable clarification summary"""
        engine = ClarificationEngine()

        clarifications = engine.detect_ambiguities("Simulate a meeting")

        summary = engine.get_clarification_summary(clarifications)

        # Should have readable summary
        assert isinstance(summary, str)
        assert len(summary) > 0

        # Should mention priority levels if clarifications exist
        if clarifications:
            assert "Critical" in summary or "Important" in summary


class TestE2EFullStack:
    """E2E tests for complete NL → Config → Execution pipeline"""

    def test_nl_to_validated_config_pipeline(self):
        """Test complete NL → validated config pipeline"""
        # 1. Start with natural language
        description = (
            "Simulate a crisis meeting with 3 astronauts and mission control. "
            "Focus on decision making under pressure. "
            "10 timepoints from explosion to resolution."
        )

        # 2. Generate config
        generator = NLConfigGenerator()
        config, confidence = generator.generate_config(description)

        # 3. Validate
        validation = generator.validate_config(config)

        # Should produce valid config
        assert validation.is_valid
        assert config is not None

        # 4. Config should be usable for simulation
        # (In real workflow, this would be passed to orchestrator)
        assert "scenario" in config
        assert "entities" in config
        assert "timepoint_count" in config
        assert "temporal_mode" in config

    def test_interactive_refinement_to_final_config(self):
        """Test complete interactive refinement to final approved config"""
        # 1. Start interactive refinement
        refiner = InteractiveRefiner()

        description = "Simulate the Constitutional Convention with 10 delegates"

        # 2. Detect clarifications
        result = refiner.start_refinement(description)

        # 3. Answer any clarifications
        if result["clarifications_needed"]:
            answers = {
                "timepoint_count": "15",
                "focus": "dialog, decision_making",
                "outputs": "dialog, decisions"
            }
            result = refiner.answer_clarifications(answers)

        # 4. Preview config
        preview = refiner.preview_config(format="summary")
        assert len(preview) > 0

        # 5. Make adjustments if needed
        if result["config"]["timepoint_count"] != 20:
            result = refiner.adjust_config(
                {"timepoint_count": 20},
                regenerate=False
            )

        # 6. Approve final config
        final_config = refiner.approve_config()

        # Should have complete, valid config
        assert final_config is not None
        assert final_config["timepoint_count"] == 20

        # 7. Export trace for debugging
        trace = refiner.export_refinement_trace()
        assert trace["final_config"] == final_config

    def test_error_recovery_workflow(self):
        """Test error recovery and retry workflow"""
        generator = NLConfigGenerator()

        # Even with vague description, mock mode should generate something
        vague = "Simulate a thing"

        config, confidence = generator.generate_config(vague)

        # Should still produce config (in mock mode)
        assert config is not None

        # May have lower confidence
        assert confidence >= 0.0

    def test_multiple_config_generations(self):
        """Test generating multiple different configs"""
        generator = NLConfigGenerator()

        configs = []

        descriptions = [
            "Simulate a board meeting with 5 executives. 10 timepoints.",
            "Simulate Apollo 13 crisis with 4 people. 8 timepoints.",
            "Simulate a negotiation with 2 parties. 5 timepoints."
        ]

        for desc in descriptions:
            config, confidence = generator.generate_config(desc)
            configs.append(config)

            # Each should be valid
            validation = generator.validate_config(config)
            assert validation.is_valid

        # Should have generated 3 configs
        assert len(configs) == 3

        # Configs should differ
        # (at least in scenario or entity count)
        assert configs[0] != configs[1]
        assert configs[1] != configs[2]


@pytest.mark.integration
class TestE2EIntegrationWithExistingSystem:
    """Integration tests with existing Timepoint-Daedalus system"""

    def test_nl_generated_config_structure_matches_system(self):
        """Test NL-generated configs match expected system structure"""
        generator = NLConfigGenerator()

        description = "Simulate a meeting with 5 people. 10 timepoints. Focus on dialog."

        config, _ = generator.generate_config(description)

        # Verify structure matches what orchestrator expects
        required_fields = [
            "scenario",
            "entities",
            "timepoint_count",
            "temporal_mode",
            "focus",
            "outputs"
        ]

        for field in required_fields:
            assert field in config, f"Missing required field: {field}"

        # Verify entity structure
        for entity in config["entities"]:
            assert "name" in entity
            assert "role" in entity

    def test_nl_config_temporal_modes_valid(self):
        """Test NL-generated configs use valid temporal modes"""
        generator = NLConfigGenerator()

        description = "Simulate a scenario with 3 people. 5 timepoints."

        config, _ = generator.generate_config(description)

        # Verify temporal mode is valid
        valid_modes = ["pearl", "directorial", "nonlinear", "branching", "cyclical"]
        assert config["temporal_mode"] in valid_modes

    def test_nl_config_focus_areas_valid(self):
        """Test NL-generated configs use valid focus areas"""
        generator = NLConfigGenerator()

        description = "Simulate a meeting with 3 people. Focus on dialog and decisions. 5 timepoints."

        config, _ = generator.generate_config(description)

        # Verify focus areas are valid
        valid_focus = {
            "dialog", "decision_making", "relationships",
            "stress_responses", "knowledge_propagation"
        }

        for focus in config["focus"]:
            assert focus in valid_focus, f"Invalid focus area: {focus}"

    def test_nl_config_outputs_valid(self):
        """Test NL-generated configs use valid output types"""
        generator = NLConfigGenerator()

        description = "Simulate a meeting with 3 people. Output dialog and decisions. 5 timepoints."

        config, _ = generator.generate_config(description)

        # Verify outputs are valid
        valid_outputs = {"dialog", "decisions", "relationships", "knowledge_flow"}

        for output in config["outputs"]:
            assert output in valid_outputs, f"Invalid output type: {output}"
