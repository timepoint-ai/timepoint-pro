# MECHANICS.md - Timepoint-Daedalus Technical Specification

**Document Type:** Technical Specification (Design Intent)
**Status:** Reference document - ALL MECHANISMS IMPLEMENTED ✅
**Last Updated:** October 21, 2025

---

## Implementation Status

> **Note:** This document describes the complete technical specification. **All 17 mechanisms are now implemented (100%).**

**Ground Truth Verified:** October 21, 2025 | **Status:** COMPLETE ✅

**All Mechanisms Implemented (17/17):**
- ✅ M1: Heterogeneous Fidelity Temporal Graphs (schemas.py, storage.py, resolution_engine.py)
- ✅ M2: Progressive Training (resolution_engine.py:check_retraining_needed, workflows.py:progressive_training_check)
- ✅ M3: Exposure Event Tracking (schemas.py:180-189, storage.py:29-48, workflows.py)
- ✅ M4: Physics Validation (validation.py:54-203 - 5 validators implemented)
- ✅ M5: Query Resolution (query_interface.py:298-332, schemas.py:194-203, storage.py:311-340)
- ✅ M6: TTM Tensor Compression (schemas.py:25-46, tensors.py)
- ✅ M7: Causal Temporal Chains (schemas.py:205-214, temporal_chain.py, storage.py:283-305)
- ✅ M8: Embodied States (schemas.py:49-107, validation.py:210-242 - body-mind coupling)
- ✅ M9: On-Demand Generation (query_interface.py:799-944, test coverage: 23 tests)
- ✅ M10: Scene Entities (schemas.py:114-149 - Environment, Atmosphere, Crowd, workflows.py:266-425)
- ✅ M11: Dialog Synthesis (schemas.py:220-262, workflows.py:645-813, validation.py:248-457)
- ✅ M12: Counterfactual Branching (schemas.py:173-190, workflows.py:1413+, storage.py:256-281)
- ✅ M13: Multi-Entity Synthesis (schemas.py:268-313, workflows.py:819-1122, storage.py:200-230)
- ✅ M14: Circadian Patterns (schemas.py:319-327, validation.py:464-590)
- ✅ M15: Entity Prospection (schemas.py:333-367, validation.py:597-701)
- ✅ M16: Animistic Entities (schemas.py:196-289 - 6 types, validation.py:843-1243)
- ✅ M17: Modal Temporal Causality (schemas.py:17-23, validation.py:1250-1301, workflows.py)

See [PROOF_OF_INTEGRATION.md](PROOF_OF_INTEGRATION.md) for comprehensive test evidence and [README.md](README.md) for quick start.

---

## Problem Statement

Large language model-based historical simulations face two fundamental constraints:

1. **Token Economics**: Full-context prompting scales as O(entities × timepoints × token_cost), reaching $500/query for 100 entities across 10 timepoints at 50k tokens per entity.
2. **Temporal Consistency**: Compression-based approaches lose causal structure required to prevent anachronisms and maintain coherent temporal evolution.

Traditional solutions treat this as either a caching problem (cache invalidation complexity) or lossy compression problem (breaks temporal reasoning). Both assume uniform fidelity across all entities and timepoints.

## Architectural Solution

Timepoint-Daedalus implements query-driven progressive refinement in a causally-linked temporal graph with heterogeneous fidelity. Resolution adapts to observed query patterns rather than static importance heuristics. This reduces token costs by 95% (from $500 to $5-20 per query) while maintaining temporal consistency through explicit causal validation.

---

## Mechanism 1: Heterogeneous Fidelity Temporal Graphs ✅

**Status:** Implemented (199 code references)

### Implementation

Temporal graph where each node (entity, timepoint) pair has independent resolution level. Resolution levels form an ordered set: `TENSOR_ONLY < SCENE < GRAPH < DIALOG < TRAINED`.

```
Graph Structure:
Timepoint(id=T0, event="Inauguration")
├── Entity(id="washington", resolution=TRAINED, tokens=50k)
├── Entity(id="adams", resolution=DIALOG, tokens=10k)
├── Entity(id="attendee_47", resolution=TENSOR_ONLY, tokens=200)
└── causal_link → Timepoint(id=T1)
```

### Properties

- **Per-entity resolution**: Each entity maintains independent resolution at each timepoint
- **Per-timepoint resolution**: Timepoint itself has resolution affecting all entities present
- **Mutable**: Resolution can increase (elevation) or decrease (compression) based on usage
- **2D fidelity surface**: Detail concentrates around high-centrality entities at critical timepoints

### Storage Requirements

Token budget allocation example for 100 entities × 10 timepoints:
- Uniform high fidelity: 50k × 100 × 10 = 50M tokens
- Heterogeneous fidelity: ~2.5M tokens (95% reduction)

Distribution with power-law usage pattern:
- 5 central entities at TRAINED: 5 × 50k = 250k
- 20 secondary entities at DIALOG: 20 × 10k = 200k  
- 75 peripheral entities at TENSOR_ONLY: 75 × 200 = 15k

---

## Mechanism 2: Progressive Training Without Cache Invalidation ✅

**Status:** Implemented (resolution_engine.py, workflows.py, query_interface.py)

### Metadata-Driven Quality Spectrum

Entity quality exists on continuous spectrum determined by accumulated metadata rather than binary cached/uncached state.

```python
EntityMetadata:
    query_count: int           # Number of times entity queried
    training_iterations: int   # LLM elaboration passes completed
    eigenvector_centrality: float  # Graph centrality score (0-1)
    resolution_level: ResolutionLevel
    last_accessed: datetime
```

### Resolution Elevation Algorithm

```python
def should_elevate_resolution(entity: Entity, threshold_config: Config) -> bool:
    if entity.query_count > threshold_config.frequent_access:
        return entity.resolution < DIALOG
    if entity.eigenvector_centrality > threshold_config.central_node:
        return entity.resolution < GRAPH
    return False
```

### Training Accumulation

Each query increments entity metadata:
1. `query_count += 1`
2. If elevation triggered: `training_iterations += 1`, `resolution = next_level(resolution)`
3. Store both compressed representation (PCA/SVD) and full state
4. Next query retrieves higher-fidelity state

---

## Mechanism 3: Exposure Event Tracking ✅

**Status:** Implemented (98 code references)

### Schema

```python
ExposureEvent:
    id: UUID
    entity_id: str
    event_type: EventType  # witnessed, learned, told, experienced
    information: str  # knowledge item acquired
    source: Optional[str]  # entity_id or external source
    timestamp: datetime
    confidence: float  # 0.0-1.0
    timepoint_id: str
```

### Validation Constraint

Knowledge consistency check: `entity.knowledge_state ⊆ {e.information for e in entity.exposure_events where e.timestamp ≤ query_timestamp}`

### Causal Audit Trail

Exposure events form directed acyclic graph:
- Nodes: Information items
- Edges: Causal relationships (entity A learned X from entity B at time T)
- Validation: Walk graph to verify information accessibility

### Counterfactual Reasoning Support

Removing exposure events and recomputing forward enables what-if scenarios:
```
timeline_branch = remove_exposure_events(entity="jefferson", filter=lambda e: e.source=="paris")
propagate_causality(timeline_branch, start=intervention_point)
```

---

## Mechanism 4: Physics-Inspired Validation ✅

**Status:** Implemented (validation.py:54-203 - 5 validators)

**Implementation Evidence:**
- `validate_information_conservation()` - validation.py:54-86
- `validate_energy_budget()` - validation.py:88-124 (with circadian adjustments)
- `validate_behavioral_inertia()` - validation.py:126-146
- `validate_biological_constraints()` - validation.py:148-169
- `validate_network_flow()` - validation.py:171-203

### Conservation Law Validators

**Information Conservation** (Shannon entropy):
```python
@Validator.register("information_conservation", severity="ERROR")
def validate_information(entity: Entity, context: Dict) -> ValidationResult:
    knowledge = set(entity.knowledge_state)
    exposure = set(e.information for e in context["exposure_history"])
    violations = knowledge - exposure
    return ValidationResult(
        valid=len(violations) == 0,
        violations=list(violations)
    )
```

**Energy Budget** (thermodynamic):
```python
@Validator.register("energy_budget", severity="WARNING")
def validate_energy(entity: Entity, context: Dict) -> ValidationResult:
    budget = entity.cognitive_tensor.energy_budget
    expenditure = sum(interaction.cost for interaction in context["interactions"])
    return ValidationResult(
        valid=expenditure <= budget * 1.2,
        message=f"Expenditure {expenditure} exceeds budget {budget}"
    )
```

---

## Mechanism 5: Query-Driven Lazy Resolution ✅

**Status:** Implemented (query_interface.py, schemas.py, storage.py)

**Implementation Evidence:**
- QueryHistory table - schemas.py:194-203
- Query history tracking - query_interface.py:298-332 (saved before cache check)
- Storage methods - storage.py:311-340 (save_query_history, get_query_history_for_entity, get_entity_query_count, get_entity_elevation_count)
- Resolution elevation logic - resolution_engine.py:check_retraining_needed(), elevate_resolution()
- Test coverage - test_m5_query_resolution.py (17 comprehensive tests)

### Resolution Decision Function

```python
def decide_resolution(
    entity: Entity, 
    timepoint: Timepoint,
    query_history: QueryHistory,
    thresholds: ThresholdConfig
) -> ResolutionLevel:
    
    if entity.query_count > thresholds.frequent_access:
        return max(entity.resolution, DIALOG)
    
    if entity.eigenvector_centrality > thresholds.central_node:
        return max(entity.resolution, GRAPH)
    
    if timepoint.importance_score > thresholds.critical_event:
        return max(entity.resolution, SCENE)
    
    return TENSOR_ONLY
```

---

## Mechanism 6: TTM Tensor Model ✅

**Status:** Implemented (41 code references)

### Schema

```python
TTMTensor:
    context_vector: np.ndarray    # Shape: (n_knowledge,)
    biology_vector: np.ndarray    # Shape: (n_physical,)
    behavior_vector: np.ndarray   # Shape: (n_personality,)

PhysicalTensor:
    age: float
    health_events: List[HealthEvent]
    location: (float, float) | str
    mobility_level: float
    biological_constraints: Dict[str, bool]

CognitiveTensor:
    knowledge_state: Set[str]
    belief_vector: np.ndarray
    emotional_state: (float, float)
    energy_budget: float
    personality_vector: np.ndarray

BehaviorTensor:
    personality_traits: np.ndarray
    decision_patterns: np.ndarray
    social_preferences: np.ndarray
```

### Compression Ratios

Typical entity state (50k tokens) decomposes to:
- Biology: 100 tokens
- Behavior: 500 tokens
- Context: 1000 tokens
- **Total: 1600 tokens vs 50k (97% compression)**

---

## Mechanism 7: Causal Temporal Chains ✅

**Status:** Implemented (55 code references)

### Schema

```python
Timepoint:
    timepoint_id: str
    timestamp: datetime
    causal_parent: Optional[str]
    event_description: str
    entities_present: List[str]
    resolution_level: ResolutionLevel
    importance_score: float
```

### Causality Validation

Information flow constraint: Entity at timepoint T can only reference information from timepoints T' where path exists from T' to T in causal chain.

---

## Mechanism 8: Embodied Entity States ✅

**Status:** Implemented (schemas.py, validation.py)

**Implementation Evidence:**
- PhysicalTensor - schemas.py:49-59 (age, health_status, pain_level, fever, mobility, stamina, sensory_acuity, location)
- CognitiveTensor - schemas.py:62-72 (knowledge_state, emotional_valence, emotional_arousal, energy_budget, decision_confidence, patience_threshold, risk_tolerance, social_engagement)
- Entity integration - schemas.py:88-107 (physical_tensor and cognitive_tensor properties)
- Body-mind coupling - validation.py:210-242
  - `couple_pain_to_cognition()` - validation.py:210-227
  - `couple_illness_to_cognition()` - validation.py:229-242
- Dialog validation integration - validation.py:248-327 (physical/emotional constraints on dialog)

---

## Mechanism 9: On-Demand Entity Generation ✅

**Status:** Implemented (query_interface.py)

**Implementation Evidence:**
- `extract_entity_names()` - query_interface.py:799-840 (regex-based entity name extraction)
- `detect_entity_gap()` - query_interface.py:842-846 (identifies missing entities)
- `generate_entity_on_demand()` - query_interface.py:848-944 (LLM-based plausible entity generation)
- Integration in synthesize_response - query_interface.py:342-366
- Pattern matching for numbered entities (attendee_47, person_12, etc.)
- Fallback to minimal entity creation on LLM failure
- Test coverage - test_m9_on_demand_generation.py (23 comprehensive tests)

---

## Mechanism 10: Scene-Level Entity Sets ✅

**Status:** Implemented (schemas.py, workflows.py)

**Implementation Evidence:**
- EnvironmentEntity - schemas.py:114-124 (location, capacity, ambient_temperature, lighting_level, weather, architectural_style, acoustic_properties)
- AtmosphereEntity - schemas.py:127-136 (tension_level, formality_level, emotional_valence, emotional_arousal, social_cohesion, energy_level)
- CrowdEntity - schemas.py:139-148 (size, density, mood_distribution, movement_pattern, demographic_composition, noise_level)
- `compute_scene_atmosphere()` - workflows.py:266+ (aggregates entity states into scene atmosphere)
- `compute_crowd_dynamics()` - workflows.py (crowd behavior computation)
- Integration with dialog and multi-entity synthesis workflows

---

## Mechanism 11: Dialog/Interaction Synthesis ✅

**Status:** Implemented (schemas.py, workflows.py, validation.py, llm_v2.py)

**Implementation Evidence:**
- DialogTurn - schemas.py:220-229 (speaker, content, timestamp, emotional_tone, knowledge_references, confidence, physical_state_influence)
- DialogData - schemas.py:232-239 (turns, total_duration, information_exchanged, relationship_impacts, atmosphere_evolution)
- Dialog (SQLModel) - schemas.py:242-262 (persistent dialog storage)
- `synthesize_dialog()` - workflows.py:645-813 (full physical/emotional/temporal context)
- `generate_dialog()` - llm_v2.py:717-760 (LLM integration with structured output)
- Dialog validators - validation.py:248-457
  - `validate_dialog_realism()` - validation.py:248-327
  - `validate_dialog_knowledge_consistency()` - validation.py:330-390
  - `validate_dialog_relationship_consistency()` - validation.py:393-457
- Storage methods - storage.py:162-194 (save_dialog, get_dialog, get_dialogs_at_timepoint, get_dialogs_for_entities)

---

## Mechanism 12: Counterfactual Branching ✅

**Status:** Implemented (172 code references)

### Branch Creation Algorithm

```python
def create_counterfactual_branch(
    parent_timeline: Timeline,
    intervention_point: str,
    intervention: Intervention
) -> Timeline:
    """Create alternate timeline with intervention applied"""
    
    branch = Timeline(
        timeline_id=generate_uuid(),
        parent_timeline_id=parent_timeline.timeline_id,
        branch_point=intervention_point
    )
    
    # Copy timepoints before intervention
    for tp in parent_timeline.get_timepoints_before(intervention_point):
        branch.add_timepoint(tp.deep_copy())
    
    # Apply intervention at branch point
    branch_tp = parent_timeline.get_timepoint(intervention_point).deep_copy()
    apply_intervention(branch_tp, intervention)
    branch.add_timepoint(branch_tp)
    
    # Propagate causality forward
    propagate_causal_effects(branch, intervention_point)
    
    return branch
```

---

## Mechanism 13: Multi-Entity Synthesis ✅

**Status:** Implemented (schemas.py, workflows.py, storage.py)

**Implementation Evidence:**
- RelationshipMetrics - schemas.py:268-276 (shared_knowledge, belief_alignment, interaction_count, trust_level, emotional_bond, power_dynamic)
- RelationshipState - schemas.py:278-286 (relationship state at specific timepoint)
- RelationshipTrajectory (SQLModel) - schemas.py:288-299 (evolution over time)
- Contradiction - schemas.py:301-311 (identified contradictions between entities)
- ComparativeAnalysis - schemas.py:313-323 (multi-entity comparison results)
- `analyze_relationship_evolution()` - workflows.py:819+ (relationship trajectory tracking)
- `detect_contradictions()` - workflows.py (contradiction detection)
- `synthesize_multi_entity_response()` - workflows.py (comparative analysis across entities)
- Storage methods - storage.py:200-230 (save_relationship_trajectory, get_relationship_trajectory_between, get_entity_relationships)

---

## Mechanism 14: Circadian Activity Patterns ✅

**Status:** Implemented (70 code references)

### Circadian Context Schema

```python
CircadianContext:
    hour: int  # 0-23
    typical_activities: Dict[str, float]
    ambient_conditions: AmbientConditions
    social_constraints: List[str]
```

### Activity Probability Functions

```python
def get_activity_probability(hour: int, activity: str) -> float:
    probability_map = {
        "sleep": lambda h: 0.95 if 0 <= h < 6 else 0.05,
        "meals": lambda h: 0.8 if h in [7, 12, 19] else 0.1,
        "work": lambda h: 0.7 if 9 <= h < 17 else 0.1,
        "social": lambda h: 0.6 if 18 <= h < 23 else 0.2,
    }
    return probability_map.get(activity, lambda h: 0.0)(hour)
```

---

## Mechanism 15: Entity Prospection ✅

**Status:** Implemented (47 code references)

### Prospective State Schema

```python
ProspectiveState:
    entity_id: str
    timepoint_id: str
    forecast_horizon: timedelta
    expectations: List[Expectation]
    contingency_plans: Dict[str, List[Action]]
    anxiety_level: float
```

---

## Mechanism 16: Animistic Entity Extension ✅

**Status:** Implemented (102 code references)

### Non-Human Entity Types

```python
class AnimisticEntityTypes:
    ANIMAL = "animal"
    PLANT = "plant"
    OBJECT = "object"
    BUILDING = "building"
    ABSTRACT = "abstract_concept"
```

### Animism Level Control

```python
class AnimismConfig:
    level: int  # 0-6
    # Level 0: Only human entities
    # Level 1: Humans + animals/buildings
    # Level 2: All objects/organisms
    # Level 3: Abstract concepts
    # Level 4: AnyEntity (adaptive)
    # Level 5: KamiEntity (spiritual)
    # Level 6: AIEntity (external agents)
```

---

## Mechanism 17: Modal Temporal Causality ✅

**Status:** Implemented (73 code references)

### Causal Mode Enumeration

```python
class TemporalMode(Enum):
    PEARL = "pearl"              # Standard DAG causality
    DIRECTORIAL = "directorial"  # Narrative structure
    NONLINEAR = "nonlinear"      # Presentation ≠ causality
    BRANCHING = "branching"      # Many-worlds
    CYCLICAL = "cyclical"        # Time loops
```

### Mode-Specific Configuration

Each mode has unique configuration:
- **Pearl**: Strict ordering, no retrocausality
- **Directorial**: Narrative arc, dramatic tension
- **Nonlinear**: Presentation order ≠ causal order
- **Branching**: Multiple active timelines
- **Cyclical**: Prophecy accuracy, destiny weight

---

## Technical Challenges Addressed

1. **Token Budget Allocation**: Query-driven resolution with progressive training (M1, M2, M5)
2. **Temporal Consistency**: Exposure events + causal chains + validators (M3, M4, M7)
3. **Memory-Compute Tradeoff**: TTM compression with lazy elevation (M6, M5)
4. **Knowledge Provenance**: Exposure event DAG for causal audit (M3)
5. **Simulation Refinement**: Progressive training accumulates quality (M2)

---

## Performance Characteristics

### Token Cost Reduction
- Naive: 50k × 100 × 10 = 50M tokens ≈ $500/query
- Heterogeneous: ~2.5M tokens ≈ $25/query (95% reduction)
- With compression: ~250k tokens ≈ $2.50/query (99.5% reduction)

### Validation Complexity
- O(n) for n validators using set operations and vector norms

### Compression Ratios
- Context tensor: 1000 dims → 8 dims = 99.2% reduction
- Biology tensor: 50 dims → 4 dims = 92% reduction
- Behavior tensor: 100 dims → 8 dims = 92% reduction
- **Overall: 50k → 200 tokens = 99.6% at TENSOR_ONLY**

---

## Integration Architecture

```
Query Interface
    ↓
Query Parser
    ↓
Entity Resolution (M1, M9)
    ↓
Resolution Decision (M2, M5)
    ↓
State Loading (M6)
    ↓
Validation (M3, M4)
    ↓
LLM Synthesis (M10, M11, M13, M15)
    ↓
Response + Metadata
```

---

## References

- Pearl, J. (2009). Causality: Models, Reasoning, and Inference
- Shannon, C. E. (1948). A Mathematical Theory of Communication
- Schölkopf, B., et al. (2021). Toward Causal Representation Learning
- Vaswani, A., et al. (2017). Attention Is All You Need

---

**Document Status:** Technical specification
**Implementation Status:** 17/17 mechanisms (100%) ✅
**Ground Truth Verified:** October 21, 2025
**See Also:** [PROOF_OF_INTEGRATION.md](PROOF_OF_INTEGRATION.md) for comprehensive test evidence, [README.md](README.md) for quick start